{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"SAME\"  #@param ['SAME', 'VALID' ]\n",
    "num_output_classes = 29  #@param {type: \"number\"}\n",
    "batch_size = 32  #@param {type: \"number\"}\n",
    "learning_rate = 0.001  #@param {type: \"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Define paths to train, test, and validation directories\n",
    "train_path = 'C:/Users/Dell/Downloads/GSASLdatasets/train/'\n",
    "val_path = 'C:/Users/Dell/Downloads/GSASLdatasets/val/'\n",
    "test_path = 'C:/Users/Dell/Downloads/GSASLdatasets/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 29 classes.\n",
      "Found 8700 images belonging to 29 classes.\n",
      "Found 8700 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_datagen.flow_from_directory(directory=train_path,\n",
    "                                         target_size=(img_height, img_width),\n",
    "                                         batch_size=batch_size,\n",
    "                                          color_mode='grayscale',\n",
    "                                         class_mode='categorical')\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(directory=val_path,\n",
    "                                       target_size=(img_height, img_width),\n",
    "                                       batch_size=batch_size,\n",
    "                                        color_mode='grayscale',\n",
    "                                       class_mode='categorical')\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(directory=test_path,\n",
    "                                        target_size=(img_height, img_width),\n",
    "                                        batch_size=batch_size,\n",
    "                                         color_mode='grayscale',\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 files belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_path,\n",
    "    labels=\"inferred\",  # Automatically infer labels from directory structure\n",
    "    label_mode=\"categorical\",  # Use categorical labels\n",
    "    color_mode=\"grayscale\",  # Set the color mode\n",
    "    batch_size=batch_size,  # Set batch size\n",
    "    image_size=(img_height, img_width),  # Set image size\n",
    "    shuffle=True,  # Shuffle the dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_data,\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust the data types as per your dataset\n",
    "    output_shapes=([None, img_height, img_width, 1], [None, 29])\n",
    ")\n",
    "\n",
    "# Convert val_data to tf.data.Dataset\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: val_data,\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust the data types as per your dataset\n",
    "    output_shapes=([None, img_height, img_width, 1], [None, 29])\n",
    ")\n",
    "\n",
    "# Optionally shuffle and batch the datasets\n",
    "train_dataset = train_dataset.shuffle(1024).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leaky_relu_alpha = 0.2 #@param {type: \"number\"}\n",
    "dropout_rate = 0.5 #@param {type: \"number\"}\n",
    "\n",
    "def conv2d( inputs , filters , stride_size ):\n",
    "    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding ) \n",
    "    return tf.nn.leaky_relu( out , alpha=leaky_relu_alpha ) \n",
    "\n",
    "def maxpool( inputs , pool_size , stride_size ):\n",
    "    return tf.nn.max_pool2d( inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n",
    "\n",
    "def dense( inputs , weights ):\n",
    "    x = tf.nn.leaky_relu( tf.matmul( inputs , weights ) , alpha=leaky_relu_alpha )\n",
    "    return tf.nn.dropout( x , rate=dropout_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 29\n",
    "initializer = tf.initializers.glorot_uniform()\n",
    "def get_weight( shape , name ):\n",
    "    return tf.Variable( initializer( shape ) , name=name , trainable=True , dtype=tf.float32 )\n",
    "\n",
    "shapes = [\n",
    "    [ 3 , 3 , 1 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 16 ] , \n",
    "    [ 3 , 3 , 16 , 32 ] , \n",
    "    [ 3 , 3 , 32 , 32 ] ,\n",
    "    [ 3 , 3 , 32 , 64 ] , \n",
    "    [ 3 , 3 , 64 , 64 ] ,\n",
    "    [ 3 , 3 , 64 , 128 ] , \n",
    "    [ 3 , 3 , 128 , 128 ] ,\n",
    "    [ 3 , 3 , 128 , 256 ] , \n",
    "    [ 3 , 3 , 256 , 256 ] ,\n",
    "    [ 3 , 3 , 256 , 512 ] , \n",
    "    [ 3 , 3 , 512 , 512 ] ,\n",
    "    [ 8192 , 3600 ] , \n",
    "    [ 3600 , 2400 ] ,\n",
    "    [ 2400 , 1600 ] , \n",
    "    [ 1600 , 800 ] ,\n",
    "    [ 800 , 64 ] ,\n",
    "    [ 64 , output_classes ] ,\n",
    "]\n",
    "\n",
    "weights = []\n",
    "for i in range( len( shapes ) ):\n",
    "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model( x ) :\n",
    "    x = tf.cast( x , dtype=tf.float32 )\n",
    "    c1 = conv2d( x , weights[ 0 ] , stride_size=1 ) \n",
    "    c1 = conv2d( c1 , weights[ 1 ] , stride_size=1 ) \n",
    "    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c2 = conv2d( p1 , weights[ 2 ] , stride_size=1 )\n",
    "    c2 = conv2d( c2 , weights[ 3 ] , stride_size=1 ) \n",
    "    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c3 = conv2d( p2 , weights[ 4 ] , stride_size=1 ) \n",
    "    c3 = conv2d( c3 , weights[ 5 ] , stride_size=1 ) \n",
    "    p3 = maxpool( c3 , pool_size=2 , stride_size=2 )\n",
    "    \n",
    "    c4 = conv2d( p3 , weights[ 6 ] , stride_size=1 )\n",
    "    c4 = conv2d( c4 , weights[ 7 ] , stride_size=1 )\n",
    "    p4 = maxpool( c4 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c5 = conv2d( p4 , weights[ 8 ] , stride_size=1 )\n",
    "    c5 = conv2d( c5 , weights[ 9 ] , stride_size=1 )\n",
    "    p5 = maxpool( c5 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    c6 = conv2d( p5 , weights[ 10 ] , stride_size=1 )\n",
    "    c6 = conv2d( c6 , weights[ 11 ] , stride_size=1 )\n",
    "    p6 = maxpool( c6 , pool_size=2 , stride_size=2 )\n",
    "\n",
    "    flatten = tf.reshape( p6 , shape=( tf.shape( p6 )[0] , -1 ))\n",
    "\n",
    "    d1 = dense( flatten , weights[ 12 ] )\n",
    "    d2 = dense( d1 , weights[ 13 ] )\n",
    "    d3 = dense( d2 , weights[ 14 ] )\n",
    "    d4 = dense( d3 , weights[ 15 ] )\n",
    "    d5 = dense( d4 , weights[ 16 ] )\n",
    "    logits = tf.matmul( d5 , weights[ 17 ] )\n",
    "\n",
    "    return tf.nn.softmax( logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf.nn.conv2d\" (type TFOpLambda).\n\nDepth of input (1) is not a multiple of input depth of filter (3) for '{{node tf.nn.conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, tf.nn.conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,100,100,1], [3,3,3,16].\n\nCall arguments received by layer \"tf.nn.conv2d\" (type TFOpLambda):\n  • input=tf.Tensor(shape=(None, 100, 100, 1), dtype=float32)\n  • filters=<tf.Variable 'weight0:0' shape=(3, 3, 3, 16) dtype=float32>\n  • strides=['1', '1', '1', '1']\n  • padding='SAME'\n  • data_format=NHWC\n  • dilations=None\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m ,\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Define your input shape\u001b[39;00m\n\u001b[0;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m keras_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel\u001b[39m( x ) :\n\u001b[0;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast( x , dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32 )\n\u001b[1;32m----> 3\u001b[0m     c1 \u001b[38;5;241m=\u001b[39m \u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m     c1 \u001b[38;5;241m=\u001b[39m conv2d( c1 , weights[ \u001b[38;5;241m1\u001b[39m ] , stride_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m ) \n\u001b[0;32m      5\u001b[0m     p1 \u001b[38;5;241m=\u001b[39m maxpool( c1 , pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m , stride_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m )\n",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m, in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, stride_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv2d\u001b[39m( inputs , filters , stride_size ):\n\u001b[1;32m----> 5\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu( out , alpha\u001b[38;5;241m=\u001b[39mleaky_relu_alpha )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[1;34m(self, op, args, kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(x, keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten([args, kwargs])\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTFOpLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.nn.conv2d\" (type TFOpLambda).\n\nDepth of input (1) is not a multiple of input depth of filter (3) for '{{node tf.nn.conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, tf.nn.conv2d/Conv2D/ReadVariableOp)' with input shapes: [?,100,100,1], [3,3,3,16].\n\nCall arguments received by layer \"tf.nn.conv2d\" (type TFOpLambda):\n  • input=tf.Tensor(shape=(None, 100, 100, 1), dtype=float32)\n  • filters=<tf.Variable 'weight0:0' shape=(3, 3, 3, 16) dtype=float32>\n  • strides=['1', '1', '1', '1']\n  • padding='SAME'\n  • data_format=NHWC\n  • dilations=None\n  • name=None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "input_shape = (100 ,100, 1)  # Define your input shape\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "outputs = model(inputs)\n",
    "keras_model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( pred , target ):\n",
    "    return tf.losses.categorical_crossentropy( target , pred )\n",
    "\n",
    "optimizer = tf.optimizers.Adam( learning_rate )\n",
    "\n",
    "# def train_step( model, inputs , outputs ):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         current_loss = loss( model( inputs ), outputs)\n",
    "#     grads = tape.gradient( current_loss , weights )\n",
    "#     optimizer.apply_gradients( zip( grads , weights ) )\n",
    "#     print( tf.reduce_mean( current_loss ) )\n",
    "# Define your train_step function\n",
    "def train_step(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\3881182609.py\", line 8, in train_step  *\n        predictions = model(images)\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\814714352.py\", line 3, in model  *\n        c1 = conv2d( x , weights[ 0 ] , stride_size=1 )\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\1590981331.py\", line 5, in conv2d  *\n        out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding )\n\n    ValueError: Depth of input (1) is not a multiple of input depth of filter (3) for '{{node Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](images, Conv2D/ReadVariableOp)' with input shapes: [32,100,100,1], [3,3,3,16].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, tf\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Cast images to float32\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(labels, tf\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Cast labels to float32\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     25\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)  \u001b[38;5;66;03m# Calculate average epoch loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileojogoinv.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(images, labels, model, optimizer, loss_function)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss_function), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(predictions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileonc12byl.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__model\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), fscope)\n\u001b[1;32m---> 11\u001b[0m c1 \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m c1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(conv2d), (ag__\u001b[38;5;241m.\u001b[39mld(c1), ag__\u001b[38;5;241m.\u001b[39mld(weights)[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mdict\u001b[39m(stride_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     13\u001b[0m p1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(maxpool), (ag__\u001b[38;5;241m.\u001b[39mld(c1),), \u001b[38;5;28mdict\u001b[39m(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file90wim2it.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__conv2d\u001b[1;34m(inputs, filters, stride_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\3881182609.py\", line 8, in train_step  *\n        predictions = model(images)\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\814714352.py\", line 3, in model  *\n        c1 = conv2d( x , weights[ 0 ] , stride_size=1 )\n    File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16228\\1590981331.py\", line 5, in conv2d  *\n        out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding )\n\n    ValueError: Depth of input (1) is not a multiple of input depth of filter (3) for '{{node Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](images, Conv2D/ReadVariableOp)' with input shapes: [32,100,100,1], [3,3,3,16].\n"
     ]
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# # Define a function for the training step\n",
    "# @tf.function\n",
    "# def train_step(images, labels, model, optimizer, loss_function):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(images)\n",
    "#         loss = loss_function(labels, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # Set the number of epochs\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     for images, labels in train_dataset:  # Assuming you have your training dataset\n",
    "#         images = tf.cast(images, tf.float32)  # Cast images to float32\n",
    "#         labels = tf.cast(labels, tf.float32)  # Cast labels to float32\n",
    "#         loss = train_step(images, labels, model, optimizer, loss_function)\n",
    "#         epoch_loss += loss\n",
    "#     average_loss = epoch_loss / len(train_dataset)  # Calculate average epoch loss\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m reduce_learning_rate \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(train_data,\n\u001b[0;32m      8\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39mval_data,\n\u001b[0;32m      9\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     10\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, reduce_learning_rate],\n\u001b[0;32m     11\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from keras.utils import plot_model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, restore_best_weights=True, verbose=1)\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_accuracy', patience=3, factor=0.6, verbose=1)\n",
    "\n",
    "history = tf.keras.fit(train_data,\n",
    "                    validation_data=val_data,\n",
    "                    epochs=15,\n",
    "                    callbacks=[early_stopping, reduce_learning_rate],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
